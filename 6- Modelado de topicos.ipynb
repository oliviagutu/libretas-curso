{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"imagenes/cabecera.png\" width=\"900\" align=\"center\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modelado de tópicos (libreta de curso)\n",
    "\n",
    "## Curso Procesamiento de Lenguaje Natural \n",
    "\n",
    "### Maestría en Tecnologías de la información\n",
    "\n",
    "\n",
    "\n",
    "#### Julio Waissman Vilanova (julio.waissman@unison.mx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los métodos de modelado de tópicos funcionan de una manera diferentea los vectores de palabras. Para el modelado de tópicos vamos a asumir que tenemos un conjunto de palabras $W = \\{w_1, \\ldots, w_n\\}$ las cuales se pueden indexar (de modo que $w_i$ le corresponde el índice $i$. Por otra parte tenemos una serie de documentos $D = \\{d_1, \\ldots, d_m\\}$ donde cada documento está representado por una secuencia de $n_j$ palabras $d_j = (w^1, \\ldots, w^{m_j})$.\n",
    "\n",
    "Si se asume que tenemos $k$ tópicos, donde $k$ se establece *a priori*, la idea del modelado de tópicos es encontrar dos matrices: la *matriz palabra a tópico* (WTM) $\\Phi$ de dimensión $n \\times k$ y la *matriz tópico a documento* (TDM) $\\Theta$ $k \\times m$. Las matrices se forman de tal manera que la entrada $\\phi_{i,k}$ de $\\Phi$ represente la probabilidad que la palabra $w_i$ conociendo el tópico, mientras que la entrada $\\theta_{k, j}$ representa la probabilidad que el documento $d_j$ se trate sobre el tópico $k$.\n",
    "\n",
    "Aqui lo interesante es que no establecemos los tópicos *a priori*, si no que las matrices se forman al minimizar una funcion objetivo. Dependiendo de la función objetivo la formación de tópicos es diferente, y los métodos de obtención de $\\Phi$ y $\\Theta$ (que para nosotros van a ser transparentes) también varian de forma importante. Los dos criterios más utilizados son los que dan a lugar a los métodos de [PLSA](https://papers.nips.cc/paper/1654-learning-the-similarity-of-documents-an-information-geometric-approach-to-document-retrieval-and-categorization.pdf) y una [LDA](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), los cuales vimos en términos generales en el curso.\n",
    "\n",
    "En particula $\\Phi$ no solamente provée una especie de matriz de vectores de palabras, si no que, ademas, nos permite ver como están distribuidos los tópicos. Un tópico es definido por el conjunto de palabras con una probabilidad significativa de encontrarse en un documento, si el documento trata sobre el tópico en cuestión. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Modelado de tópicos con LSA y LDA utilizando `Gensim` \n",
    "\n",
    "Para realizar el modelado de tópicos es necesario contar con una bolsa de palabras (de preferencia utilizando la cuenta de las palabras y no únicamente el indicador) por lo que es necesario obtener, normalizar y limpiar el *corpus*. Es importante utilizar el método de BOW que provée *gensim* para realizar el modelado de tópicos.\n",
    "\n",
    "Vamos a procesar tópicos a partir de los datos de *wikipedia* sobre *políticos argentinos* que obtuvimos y tratamos  en la [primer libreta](./1- Obtencion de textos en espanyol.ipynb). Pero primero veamos algunos para ver que hay que tratar del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from random import randint\n",
    "\n",
    "archivo_pkl = 'datos/wikipedia-politicos-argentina.pkl'\n",
    "wiki_df = pd.read_pickle(archivo_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = randint(0, wiki_df.shape[0])\n",
    "print(wiki_df.loc[i,'contenido'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a normalizar el texto. Para esto, y con el fin de encontrar palabras significantes en el análisis de tópicos, no solo hay que eliminar las palabras vacías, si no las palabras que no tienen un valor importante en el discurso. Es por eso que vamos a utilizar el método de etiquetado de parte del discurso para únicamente quedarnos con las palabras que pueden ser más significativas. De nuevo, esta selección depende de lo que queramos extraer. En principio, esto pueden ser pronombres personales (`PROPN`), sustantivos (`NOUN`), adjetivos (`ADJ`) y/o verbos (`VERB`). \n",
    "\n",
    "Para esto vamos a utilizar el modulo de *spacy*, pero con el fin de hacer el tratamiento más rápido, vmos a eliminar de la serie de pasos que realiza automáticamente *spacy* la identificación de dependencias y el reconocimiento de entidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "nlp.disable_pipes('parser', 'ner')\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliza_texto(texto):    \n",
    "    return [token.lemma_ for token in nlp(texto) \n",
    "            if (token.is_alpha and \n",
    "                not token.is_stop and\n",
    "                token.pos_ in ['PROPN', 'NOUN', 'VERB', 'ADJ'])]    \n",
    "\n",
    "documentos = [normaliza_texto(documento) \n",
    "              for documento in wiki_df['contenido'].values]\n",
    "\n",
    "print(documentos[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos que hacer cuatro pasos importantes: extraer el diccionario a partir del corpus de entrenamiento; convertir los documentos a una representación de BOW; escoger el número de tópicos que queremos analizar; y por último entrenar nuestro modelo (ya sea PLSA, LDA u otro). Pero primero vamos a sacar un documento del corpus para fines de estimación *a posteriori*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a sacar un documento del corpus\n",
    "ind_ejemplo = randint(0, len(documentos) - 1)\n",
    "ejemplo = documentos.pop(ind_ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models, corpora\n",
    "\n",
    "# Número de tópicos\n",
    "n_topicos = 20\n",
    "\n",
    "# Genera el diccionario de palabras\n",
    "diccionario = corpora.Dictionary(documentos)\n",
    "diccionario.filter_extremes(no_below=5, no_above=0.3)\n",
    " \n",
    "# Extrae las características en forma de BOW\n",
    "corpus = [diccionario.doc2bow(doc) for doc in documentos]\n",
    "    \n",
    "# Genera el modelo LDA\n",
    "modelo_lda = models.LdaModel(corpus=corpus, num_topics=n_topicos, id2word=diccionario, iterations=100, passes=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahor podemos ver algunas cosas que nos permitan analizar nuestro corpus, en primer lugar, como se define a grandes razgos cada tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, topico) in modelo_lda.print_topics(num_topics=n_topicos, num_words=5):\n",
    "    print(10 * \"-\" + \"topico {}\".format(i) + 20 * \"-\")\n",
    "    print(topico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a ver como se clasificaría en los diferentes tópicos el documento que no utilizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_ejemplo = wiki_df.loc[ind_ejemplo, 'contenido']\n",
    "print(\"El artículo \\n<<{}>>\\n\\n tiene los siguientes tópicos:\".format(texto_ejemplo))\n",
    "\n",
    "topicos_ejemplo = modelo_lda[diccionario.doc2bow(ejemplo)]\n",
    "topicos_ejemplo.sort(key=lambda x:x[1], reverse=True)\n",
    "for (topico, peso) in topicos_ejemplo:\n",
    "    print(\"\\tTopico {} con un peso de {}\".format(topico, peso))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una manera de revisar la calidad del modelo es a partir de la perplejidad de uno o varios documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pow\n",
    "\n",
    "P = modelo_lda.log_perplexity(corpus)\n",
    "pow(2, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro método es analizando la [coherencia de los tópicos](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf), ponemos algunos resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "cm = CoherenceModel(model=modelo_lda, texts=documentos, coherence='c_v')\n",
    "\n",
    "print(\"Coherencia del modelo (C_V): {}\".format(cm.get_coherence()))\n",
    "for (topico, coherencia) in enumerate(cm.get_coherence_per_topic()):\n",
    "    print(\"\\tTópico {}, coherencia {}\".format(topico, coherencia))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y tambien lo podemos utilizar para analizar los tópicos revisando todas las palabras que los definen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Las palabra que definen los tópicos\\n\")\n",
    "t_palabras = cm.top_topics_as_word_lists(model=modelo_lda, dictionary=diccionario)\n",
    "for (topico, palabras) in enumerate(t_palabras):\n",
    "    print(\"{:2}-\\t{}\\n\".format(topico, ', '.join(palabras)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Visualización de tópicos con `pyLDAvis`\n",
    "\n",
    "Como vimos, explorar los modelos creados y ajustarlos para tratar de extraer mejores tópicos es una tarea ingrata si no se cuenta con herramientas de visualización. El móduo `pyLDAvis` es un clon en python de la misma herramienta (`LDAvis`) que existe en *R*. La gran ventaja es que la herramienta está diseñada para jugar bien con *jupyter*.\n",
    "\n",
    "`LDAvis` funciona únicamente con modelos que proveen el método `inference`, como LDA que la estimación de tópicos se basa en una inferencia estadística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.gensim.prepare(corpus=corpus, dictionary=diccionario, topic_model=modelo_lda)\n",
    "panel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
